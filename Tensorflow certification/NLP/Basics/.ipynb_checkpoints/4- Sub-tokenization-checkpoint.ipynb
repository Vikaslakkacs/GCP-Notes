{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Tokenization: further splitting of tokens into parts and giving them a unique id is the goal.\n",
    "#### eg: Tensorflow: Ten:1, sor:2, flow:3 This is how the data will split.\n",
    "#### Remember this is case sensitive and it includes all the punctualtions and everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb,info=tfds.load('imdb_reviews/subwords8k',with_info=True,as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previously we have the data for that we will create tokens and pad_sequences\n",
    "### tokens are converted to sub-tokens and are already built in this dataset, related to IMDB\n",
    "### When we tryto encode any text it will pick from the encoder list and assign those vector values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features are two keys Label and text\n",
    "### If you can see the keys: label has num_classes as method, and text has encoder as method.\n",
    "### within that encoder it has subwordtextencoder where we can tr encoding the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method FeaturesDict.keys of FeaturesDict({\n",
       "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
       "})>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take the text encoder and try to encode simple text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tokenizer=features['text'].encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets encode an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample='Hello! How are you doing my dear?'\n",
    "sample_encoder=sub_tokenizer.encode(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4025, 8040, 90, 693, 29, 37, 573, 82, 4946, 8043, 7992]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tokenizer.subwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_words_index=sub_tokenizer.subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets try to decode the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder=sub_tokenizer.decode(sample_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How are you doing my dear?'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets map the words and check the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025--->Hell\n",
      "8040--->o\n",
      "90--->! \n",
      "693--->How \n",
      "29--->are \n",
      "37--->you \n",
      "573--->doing \n",
      "82--->my \n",
      "4946--->dea\n",
      "8043--->r\n",
      "7992--->?\n"
     ]
    }
   ],
   "source": [
    "for i in sample_encoder:\n",
    "    \n",
    "    print(str(i) + '--->' + str(sub_tokenizer.decode([i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you observe the aobve sub-tokens are case sensitive.\n",
    "### Individually they doesn't make any sense but together they make sense\n",
    "### Lets train the model with these tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding,GlobalAveragePooling1D,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 64)          523840    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_6 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 524,237\n",
      "Trainable params: 524,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=keras.Sequential()\n",
    "model.add(Embedding(input_dim=sub_tokenizer.vocab_size,output_dim=64))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(units=6,activation=tf.nn.relu))\n",
    "model.add(Dense(units=1,activation=tf.nn.sigmoid))\n",
    "model.compile(optimizer='adam',loss=keras.losses.BinaryCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets create train data and test data for model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=imdb['train'],imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat,train_label=[],[]\n",
    "test_cat,test_label=[],[]\n",
    "for cat,label in train_data:\n",
    "    train_cat.append(cat)\n",
    "    train_label.append(label.numpy())\n",
    "    \n",
    "for cat,label in test_data:\n",
    "    test_cat.append(cat)\n",
    "    test_label.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=np.array(train_label)\n",
    "test_label=np.array(test_label)\n",
    "#train_cat=np.asarray(train_cat)\n",
    "#test_cat=np.asarray(test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(163,), dtype=int64, numpy=\n",
       "array([  62,   18,   41,  604,  927,   65,    3,  644, 7968,   21,   35,\n",
       "       5096,   36,   11,   43, 2948, 5240,  102,   50,  681, 7862, 1244,\n",
       "          3, 3266,   29,  122,  640,    2,   26,   14,  279,  438,   35,\n",
       "         79,  349,  384,   11, 1991,    3,  492,   79,  122,  188,  117,\n",
       "         33, 4047, 4531,   14,   65, 7968,    8, 1819, 3947,    3,   62,\n",
       "         27,    9,   41,  577, 5044, 2629, 2552, 7193, 7961, 3642,    3,\n",
       "         19,  107, 3903,  225,   85,  198,   72,    1, 1512,  738, 2347,\n",
       "        102, 6245,    8,   85,  308,   79, 6936, 7961,   23, 4981, 8044,\n",
       "          3, 6429, 7961, 1141, 1335, 1848, 4848,   55, 3601, 4217, 8050,\n",
       "          2,    5,   59, 3831, 1484, 8040, 7974,  174, 5773,   22, 5240,\n",
       "        102,   18,  247,   26,    4, 3903, 1612, 3902,  291,   11,    4,\n",
       "         27,   13,   18, 4092, 4008, 7961,    6,  119,  213, 2774,    3,\n",
       "         12,  258, 2306,   13,   91,   29,  171,   52,  229,    2, 1245,\n",
       "       5790,  995, 7968,    8,   52, 2948, 5240, 8039, 7968,    8,   74,\n",
       "       1249,    3,   12,  117, 2438, 1369,  192,   39, 7975])>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    # Use the version pre-encoded with an ~8k vocabulary.\n",
    "    'imdb_reviews/subwords8k', \n",
    "    # Return the train/test datasets as a tuple.\n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
    "    as_supervised=True,\n",
    "    # Also return the `info` structure. \n",
    "    with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((None,), ()), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padded Batch is similar to Sequencing and putting all the values in sync\n",
    "##### Eg: if the value is 32 then all the length equals to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch=train_data.shuffle(1000).padded_batch(batch_size=32,padded_shapes=([None], []))\n",
    "#test_batch=test_data.padded_batch(32)\n",
    "test_batches = (\n",
    "    test_data\n",
    "    .padded_batch(batch_size=32,padded_shapes=([None], [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text: [  62   18   41  604  927   65    3  644 7968   21]\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "for train_example, train_label in train_data.take(1):\n",
    "    print('Encoded text:', train_example[:10].numpy())\n",
    "    print('Label:', train_label.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (32, 974)\n",
      "label shape: (32,)\n",
      "Batch shape: (32, 697)\n",
      "label shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "for example_batch, label_batch in train_batch.take(2):\n",
    "    print(\"Batch shape:\", example_batch.shape)\n",
    "    print(\"label shape:\", label_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.6076 - accuracy: 0.7004 - val_loss: 0.4681 - val_accuracy: 0.8025\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.3565 - accuracy: 0.8723 - val_loss: 0.3608 - val_accuracy: 0.8418\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 25s 33ms/step - loss: 0.2717 - accuracy: 0.9040 - val_loss: 0.3264 - val_accuracy: 0.8648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f264764af10>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_batch,epochs=3,validation_data=test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
