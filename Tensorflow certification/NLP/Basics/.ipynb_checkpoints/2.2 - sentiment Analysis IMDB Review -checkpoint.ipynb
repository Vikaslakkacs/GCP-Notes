{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "### We will train the model using IMDB Reviews and predict whether it is a bad or good review\n",
    "#### This can be done by using Embeddings: Embedding is a layer where it has its own weights defined and when the words or paragraphs are passed, it will seggregate and push those into respective cluster group.\n",
    "##### Tips: We have to convert the labels from tensors and then into lists ansd then into numpy arrays\n",
    "##### Tip: We have to convert the sentences from tensors to numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews (80.23 MiB) to /home/jupyter/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450f5101a5194b3c82f45e871d267661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30b0c48076e4f2db90d7973f28dac85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/jupyter/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteB28MMM/imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1b1b35752b4b1ea38f714a20ace87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/jupyter/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteB28MMM/imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace23ab44e694426a4a00dd7044d4b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/jupyter/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteB28MMM/imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6fadee921b459b8fe8a3f163984f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/jupyter/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\r"
     ]
    }
   ],
   "source": [
    "imdb,info=tfds.load('imdb_reviews',with_info=True,as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data,testing_data=imdb['train'],imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list=list(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list[4598][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have got the values now. Lets create Training_categories and training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_category=[]\n",
    "training_label=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat,label in training_data:\n",
    "    training_category.append(str(cat.numpy()))\n",
    "    training_label.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets do the same with testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_category=[]\n",
    "test_label=[]\n",
    "for cat,label in testing_data:\n",
    "    test_category.append(str(cat.numpy()))\n",
    "    test_label.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have category into string but labels in list. Lets convert them into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_label_numpy=np.array(training_label)\n",
    "test_label_numpy=np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_label_numpy=training_label_numpy.reshape(-1,1)\n",
    "test_label_numpy=test_label_numpy.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_label_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_label_numpy[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel\\'s absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven\\'t laughed this hard since I saw THE FULL MONTY. (And, even then, I don\\'t think I laughed quite this hard... So to speak.) Tukel\\'s talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there\\'s none of the over-the-top scenery chewing one might\\'ve expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_category[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Its Show time !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us load some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "max_length=120\n",
    "embedded_dim=16\n",
    "oov_text='<OOV>'\n",
    "padding='post'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=keras.preprocessing.text.Tokenizer(num_words=vocab_size,oov_token='<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization.fit_on_texts(training_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=tokenization.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['surprise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texts to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence=tokenization.texts_to_sequences(training_category)\n",
    "test_sequence=tokenization.texts_to_sequences(test_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sequence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad_sequences= keras.preprocessing.sequence.pad_sequences(train_sequence,maxlen=max_length,\n",
    "                                                                padding=padding)\n",
    "test_pad_sequences=keras.preprocessing.sequence.pad_sequences(test_sequence,maxlen=max_length,\n",
    "                                                             padding=padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding,Flatten,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=keras.Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embedded_dim,input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=6,activation=tf.nn.relu))\n",
    "model.add(Dense(units=1,activation=tf.nn.sigmoid))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 10s 416us/sample - loss: 0.4797 - accuracy: 0.7438 - val_loss: 0.3412 - val_accuracy: 0.8482\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 10s 387us/sample - loss: 0.2189 - accuracy: 0.9173 - val_loss: 0.3415 - val_accuracy: 0.8512\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 9s 367us/sample - loss: 0.0903 - accuracy: 0.9771 - val_loss: 0.4148 - val_accuracy: 0.8425\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 10s 388us/sample - loss: 0.0279 - accuracy: 0.9958 - val_loss: 0.4884 - val_accuracy: 0.8387\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 10s 412us/sample - loss: 0.0105 - accuracy: 0.9988 - val_loss: 0.5409 - val_accuracy: 0.8392\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 9s 375us/sample - loss: 0.0056 - accuracy: 0.9993 - val_loss: 0.5932 - val_accuracy: 0.8378\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 9s 369us/sample - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.6585 - val_accuracy: 0.8340\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 9s 373us/sample - loss: 9.7575e-04 - accuracy: 1.0000 - val_loss: 0.6983 - val_accuracy: 0.8364\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 9s 376us/sample - loss: 5.0095e-04 - accuracy: 1.0000 - val_loss: 0.7291 - val_accuracy: 0.8352\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 9s 345us/sample - loss: 1.9852e-04 - accuracy: 1.0000 - val_loss: 0.7615 - val_accuracy: 0.8373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9845a7ad0>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=10\n",
    "model.fit(train_pad_sequences,training_label_numpy,epochs=epochs,validation_data=(test_pad_sequences,test_label_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets create another model but not with Flatten layer but with Global Average Pooling1D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 85        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 160,091\n",
      "Trainable params: 160,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=keras.Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embedded_dim,input_length=max_length))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(units=5,activation=tf.nn.relu))\n",
    "model.add(Dense(units=1,activation=tf.nn.sigmoid))\n",
    "model.compile(optimizer='adam',loss=keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 11s 422us/sample - loss: 0.6855 - accuracy: 0.5238 - val_loss: 0.6454 - val_accuracy: 0.6817\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 11s 456us/sample - loss: 0.5971 - accuracy: 0.8207 - val_loss: 0.5839 - val_accuracy: 0.8396\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 10s 385us/sample - loss: 0.5666 - accuracy: 0.8746 - val_loss: 0.5759 - val_accuracy: 0.8393\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 10s 407us/sample - loss: 0.5555 - accuracy: 0.8948 - val_loss: 0.5727 - val_accuracy: 0.8490\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 7s 295us/sample - loss: 0.5477 - accuracy: 0.9121 - val_loss: 0.5716 - val_accuracy: 0.8562\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 10s 397us/sample - loss: 0.5418 - accuracy: 0.9238 - val_loss: 0.5714 - val_accuracy: 0.8522\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 10s 381us/sample - loss: 0.5370 - accuracy: 0.9328 - val_loss: 0.5717 - val_accuracy: 0.8539\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 9s 351us/sample - loss: 0.5333 - accuracy: 0.9400 - val_loss: 0.5724 - val_accuracy: 0.8510\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 9s 365us/sample - loss: 0.5302 - accuracy: 0.9455 - val_loss: 0.5730 - val_accuracy: 0.8521\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 10s 413us/sample - loss: 0.5279 - accuracy: 0.9493 - val_loss: 0.5736 - val_accuracy: 0.8507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff94fda3550>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_pad_sequences,training_label_numpy,epochs=epochs,validation_data=(test_pad_sequences,test_label_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a nice model. Now if we want to visualise the word embedding, Below are the steps to be followed\n",
    "#### 1. Take out the weights of the embedding layer via model.layer.weights\n",
    "#### 2. There you will find the list of weights for every word present\n",
    "#### 3. In the above problem we have 10000 words and we will find weights for all the 10k words.\n",
    "#### 4. We will create two files where we ill store weights in one file and respectve words for those weights in another.\n",
    "#### 5. As we have word_index but as a values we have numbers. We need to reverse the dictionary and then assign the weights to the values.\n",
    "#### 6. We will download them and upload in the projector.tensorflow website to view the cluster of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE will reverse the word index first\n",
    "reverse_word_index=dict([(i,w) for (w,i) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'socialist'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index[9999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will create two files with format as tsv\n",
    "### we wil load the weights and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will pick  the weights first\n",
    "layer=model.layers[0]\n",
    "weights_list=layer.get_weights()\n",
    "arr_weights=weights_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01676033, -0.10503941, -0.28673616, -0.0178824 ,  0.0012398 ,\n",
       "        0.27213126, -0.02414835, -0.20349813,  0.04617301,  0.09460002,\n",
       "        0.03749573,  0.20931901, -0.01966789,  0.02536215,  0.03766212,\n",
       "        0.10380462], dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_file=io.open('word_file.tsv','w',encoding='utf-8')\n",
    "vector_file=io.open('vector_file.tsv','w',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(vocab_size):\n",
    "    ### As we reversed the dictionary, we wond have value of 0 as it is not index it is value.\n",
    "    ### So the values starts from 1.\n",
    "    word_file.write(reverse_word_index[i+1])\n",
    "    ### As it these are weights we have to convert them to string and then \n",
    "    ### append with tab seperator for every weight and then we ill append new line for next word weight\n",
    "    vector_file.write('\\t'.join([str(x) for x in arr_weights[i]]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_file.close()\n",
    "vector_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.016760325\\t-0.10503941\\t-0.28673616\\t-0.017882396\\t0.0012398048\\t0.27213126\\t-0.024148349\\t-0.20349813\\t0.046173014\\t0.09460002\\t0.03749573\\t0.20931901\\t-0.019667894\\t0.025362145\\t0.03766212\\t0.10380462\\n'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\t'.join([str(x) for x in arr_weights[1]]) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ 0.00920381 -0.01133953 -0.1318373  -0.02579551  0.0007072   0.10600454\\n -0.05109627 -0.06589282  0.00291154  0.03623795 -0.02652154  0.05861582\\n  0.03622925 -0.01015301 -0.04672671  0.01281283]\\t[ 0.01676033 -0.10503941 -0.28673616 -0.0178824   0.0012398   0.27213126\\n -0.02414835 -0.20349813  0.04617301  0.09460002  0.03749573  0.20931901\\n -0.01966789  0.02536215  0.03766212  0.10380462]\\t[ 0.14546916  0.03354151 -0.19611074 -0.14200978  0.11267764  0.24479473\\n  0.00029444 -0.06537913 -0.04477925 -0.03161895 -0.09601128  0.13920079\\n  0.0627014  -0.11746637 -0.07766797  0.06718884]\\n'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### https://projector.tensorflow.org/ : Here we will upload both the files and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=['hey','hello','hi']\n",
    "x=[]\n",
    "for i in z:\n",
    "   x.append() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
