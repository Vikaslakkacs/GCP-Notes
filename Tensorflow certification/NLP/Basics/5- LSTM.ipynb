{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will implement LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb,info=tfds.load('imdb_reviews/subwords8k',with_info=True,as_supervised=True)\n",
    "features=info.features['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_token=features.encoder\n",
    "subwords=sub_token.subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=imdb['train'], imdb['test']\n",
    "\n",
    "train_batch=(train_data.shuffle(1000).padded_batch(64,tf.compat.v1.data.get_output_shapes(train_data)))\n",
    "test_batch=(test_data.padded_batch(64,tf.compat.v1.data.get_output_shapes(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel():\n",
    "    def __init__(self,vocab_size,max_length,embed_dim,tensorflow,keras,layers):\n",
    "        self.vocab_size=vocab_size\n",
    "        self.max_length=max_length\n",
    "        self.embed_dim=embed_dim\n",
    "        self.tensorflow=tensorflow\n",
    "        self.keras=keras\n",
    "        self.layers=layers\n",
    "    \n",
    "    def single_LSTM(self):\n",
    "        model=self.keras.Sequential()\n",
    "        model.add(self.layers.Embedding(input_dim=vocab_size,output_dim=embed_dim))\n",
    "        model.add(self.layers.Bidirectional(self.layers.LSTM(embed_dim)))\n",
    "        model.add(self.layers.Dense(units=6,activation='relu'))\n",
    "        model.add(self.layers.Dense(units=1,activation='sigmoid'))\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss=self.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "        return model\n",
    "    def classic(self):\n",
    "        model=self.keras.Sequential()\n",
    "        model.add(self.layers.Embedding(input_dim=vocab_size,output_dim=embed_dim))\n",
    "        model.add(self.layers.Flatten())\n",
    "        model.add(self.layers.Dense(units=6,activation='relu'))\n",
    "        model.add(self.layers.Dense(units=1,activation='sigmoid'))\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss=self.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def classic_Globalaverage(self):\n",
    "        model=self.keras.Sequential()\n",
    "        model.add(self.layers.Embedding(input_dim=vocab_size,output_dim=embed_dim))\n",
    "        model.add(self.layers.GlobalAveragePooling1D())\n",
    "        model.add(self.layers.Dense(units=6,activation='relu'))\n",
    "        model.add(self.layers.Dense(units=1,activation='sigmoid'))\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss=self.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "        return model \n",
    "    \n",
    "    def classic_Convo(self):\n",
    "        model=self.keras.Sequential()\n",
    "        model.add(self.layers.Embedding(input_dim=vocab_size,output_dim=embed_dim))\n",
    "        model.add(self.layers.Conv1D(128,5,activation='relu'))\n",
    "        model.add(self.layers.GlobalMaxPool1D())\n",
    "        model.add(self.layers.Dense(units=6,activation='relu'))\n",
    "        model.add(self.layers.Dense(units=1,activation='sigmoid'))\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss=self.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])\n",
    "        return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=sub_token.vocab_size\n",
    "embed_dim=16\n",
    "max_len=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=MyModel(vocab_size,embed_dim,max_len,tf,tf.keras,tf.keras.layers)\n",
    "\n",
    "lstm_model=models.single_LSTM()\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.fit(train_batch,epochs=10,validation_data=test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling1D,GlobalMaxPool1D,Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
