{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase or Vocab matching\n",
    "### Using spacy we can pass a phrase and let spacy check where the phrase is in the doc or text of string of sentence\n",
    "### This can be done in python but we there are many simple methods where we can find spaces , punctuations and anyother symbols between the words\n",
    "### In spacy this can be done using Matcher\n",
    "### Matcher basically try to match the word or phrase in the given sentence of doc.\n",
    "### We can send list of patterns and ask Matcher to find the list of phases to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import  Matcher\n",
    "## WE will load nlp.vocab\n",
    "matcher= Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will find the phrase solarpower from the text\n",
    "#### Solarpower can be solar power, solar-power or solarpower or it canbe of any kind\n",
    "#### We will define what patterns to find a word\n",
    "#### Patters are list of dictionaries with rules in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## WE will define pattern for solarpower\n",
    "pattern1= [{'LOWER':'solarpower'}]\n",
    "##Soalr power\n",
    "pattern2=[{'LOWER':'solar'}, {'LOWER':'power'}]\n",
    "#solar-power\n",
    "pattern3=[{'LOWER':'solar'}, {'IS_PUNCT':True}, {'LOWER':'power'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will create a sentence and find the matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(u\"The Solar Power industry continues to grow as demand \\\n",
    "for solarpower increases. Solar-power cars are gaining popularity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Solar Power industry continues to grow as demand for solarpower increases. Solar-power cars are gaining popularity."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is not just text this is spacy doc\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will add the patterns to matcher before finding the phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('SolarPower', None, pattern1, pattern2, pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " [[{'LOWER': 'solarpower'}],\n",
       "  [{'LOWER': 'solar'}, {'LOWER': 'power'}],\n",
       "  [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can view the patterns\n",
    "matcher.get('SolarPower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8656102463236116519, 1, 3),\n",
       " (8656102463236116519, 10, 11),\n",
       " (8656102463236116519, 13, 16)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##LEts pass this doc to Matcher and find the matched areas\n",
    "found_matches= matcher(doc)\n",
    "found_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above list of tuples consists of a matchid of each word and its placements.\n",
    "#### We can elaborate this little and can display what are the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 10 11 solarpower\n",
      "8656102463236116519 SolarPower 13 16 Solar-power\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    ## WE will take the string id from nlp\n",
    "    string_id= nlp.vocab.strings[match_id]\n",
    "    #print(string_id)\n",
    "    ## Now we will take the actual text from the doc\n",
    "    span= doc[start: end]\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the DET\n",
      "Solar PROPN\n",
      "Power PROPN\n",
      "industry NOUN\n",
      "continue VERB\n",
      "to PART\n",
      "grow VERB\n",
      "as SCONJ\n",
      "demand NOUN\n",
      "for ADP\n",
      "solarpow ADJ\n",
      "increase NOUN\n",
      ". PUNCT\n",
      "solar ADJ\n",
      "- PUNCT\n",
      "power NOUN\n",
      "car NOUN\n",
      "be AUX\n",
      "gain VERB\n",
      "popularity NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for text in doc:\n",
    "    print(text.lemma_, text.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets check with some other sentence which is little different\n",
    "### Note: Here we have different naming (solarpowered instead of solar power)\n",
    "### Lets check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(u\"'Solar-powered energy runs solar-powered cars.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches= matcher(doc)\n",
    "found_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We didnt get any results\n",
    "### In spacy while creating pattern we have token attributes\n",
    "## Other token attributes\n",
    "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
    "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
    "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
    "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
    "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
    "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
    "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
    "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
    "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
    "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From this we can pick lemmatization for powered so that we can get power\n",
    "### Lets Try!\n",
    "### In the second dict of pattern we have added one more item which is op. * this means that it will consider if there are more than zero punctuations not only one.\n",
    "### Below are some of other items we can use\n",
    "This found both two-word patterns, with and without the hyphen!\n",
    "\n",
    "The following quantifiers can be passed to the `'OP'` key:\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternlemma=[{'LOWER':'solar'}, {'IS_PUNCT':True, 'OP':'*'}, {'LEMMA':'power'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('SolarPower', None, patternlemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " [[{'LOWER': 'solarpower'}],\n",
       "  [{'LOWER': 'solar'}, {'LOWER': 'power'}],\n",
       "  [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}],\n",
       "  [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP': '*'}, {'LEMMA': 'power'}]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher.get('SolarPower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Test now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8656102463236116519, 1, 4), (8656102463236116519, 6, 9)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches= matcher(doc)\n",
    "found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Solar-powered energy runs solar-powered cars."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets check the lemmas of the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      "Solar\n",
      "-\n",
      "power\n",
      "energy\n",
      "run\n",
      "solar\n",
      "-\n",
      "power\n",
      "car\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "## This looks great\n",
    "for text in doc:\n",
    "    print(text.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WE can even mention hastags also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2=nlp(u'Hello Bangalore #HelloBglre #Gm #FeelingFit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF we are not sure about the words after the Hashtag we can simply place an empty string as wild card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternhash=[{'ORTH': '#'}, {}]\n",
    "matcher.add('Hash',None, patternhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, [[{'ORTH': '#'}, {}]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher.get('Hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13232031677015429232, 2, 4),\n",
       " (13232031677015429232, 4, 6),\n",
       " (13232031677015429232, 6, 8)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches= matcher(doc2)\n",
    "found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash\n",
      "#HelloBglre\n",
      "Hash\n",
      "#Gm\n",
      "Hash\n",
      "#FeelingFit\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id= nlp.vocab.strings[match_id]\n",
    "    print(string_id)\n",
    "    span = doc2[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will look into phrase matcher\n",
    "### instead of pattern we will pass list of values to Phasematcher instead of matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "phrasematcher=PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will take the text of REAGANOMICS and below is the link for reference\n",
    "https://en.wikipedia.org/wiki/Reaganomics\n",
    "### But we have a file which is converted to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reaganomics.txt','r', errors='ignore') as reaga:\n",
    "    raeganomics=nlp(reaga.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets take some phrases from the text and try to search those in the file\n",
    "phrase_list=['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will convert every phrase to nlp doc\n",
    "phrase_nlp= [nlp(text) for text in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[voodoo economics,\n",
       " supply-side economics,\n",
       " trickle-down economics,\n",
       " free-market economics]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOw lets add those phrase nlp to the matcher\n",
    "phrasematcher.add('Economics' , None, *phrase_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11454562835486586514, 41, 45),\n",
       " (11454562835486586514, 49, 53),\n",
       " (11454562835486586514, 54, 56),\n",
       " (11454562835486586514, 61, 65),\n",
       " (11454562835486586514, 673, 677),\n",
       " (11454562835486586514, 2986, 2990)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now lets find the phrase matcher\n",
    "found_matches= phrasematcher(raeganomics)\n",
    "found_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets check the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economics supply-side economics\n",
      "Economics trickle-down economics\n",
      "Economics voodoo economics\n",
      "Economics free-market economics\n",
      "Economics supply-side economics\n",
      "Economics trickle-down economics\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id= nlp.vocab.strings[match_id]\n",
    "    span= raeganomics[start:end]\n",
    "    print(string_id, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we want to check in which context those phrases came eg: if we want to get some words befre and after those phrases to know the context we can simply add words to start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during the 1980s. These policies are commonly associated with supply-side economics, referred to as trickle\n",
      "associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political\n",
      "economics, referred to as trickle-down economics or voodoo economics by political opponents, and\n",
      "down economics or voodoo economics by political opponents, and free-market economics by political advocates.\n",
      "\n",
      "\n",
      "At the same time he attracted a following from the supply-side economics movement, which formed in\n",
      "against institutions.[66] His policies became widely known as \"trickle-down economics\", due to the\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id= nlp.vocab.strings[match_id]\n",
    "    span= raeganomics[start-10:end+5]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets check the same with solar power\n",
    "### We will pass only solar power inthe phrase list and check whether it will detect solarpowered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(u\"'Solar-powered energy runs solar-powered cars.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list=['SolarPower','Solar Power', 'solar-powered']\n",
    "phrase_nlp= [nlp(text) for text in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrasematcher.add('SolarPower',None, *phrase_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8656102463236116519, 6, 9)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches = phrasematcher(doc)\n",
    "found_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets pass pattern in the phrase list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrasematcher.remove('SolarPower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list=['SolarPower','Solar Power']\n",
    "phrase_nlp= [nlp(text) for text in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_nlp.append(patternlemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-1f7ae681debd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphrasematcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SolarPower'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mphrase_nlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mphrasematcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.phrasematcher.PhraseMatcher.add\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "phrasematcher.add('SolarPower',None, *phrase_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WE cannot pass pattern and phrase list together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So to wrap it up\n",
    "### IF we want to find the phrases irrespective with case sensitive we can use phrase matcher as it can provide much more flexibility\n",
    "### But if we want phrases in different styles say like solar powered, solar-powered, and lemmatizations we can use Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
